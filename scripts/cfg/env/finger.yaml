name: finger
terminal_reward_name: finger
include_time: true
dtype: float64

kwargs:
  eps: 1e-6

config:
  # schedule parameters
  num_timesteps: 20_000_000

  # environment wrapper
  episode_length: 75
  action_repeat: 1

  # network parameters
  actor_grad_norm: 1.0
  critic_grad_norm: 1.0
  policy_hidden_layer_sizes: [128, 256, 128]
  value_hidden_layer_sizes: [128, 256, 128]
  tau: 0.2
  betas: [0.7, 0.95]

  # SHAC params
  entropy_cost: 1e-4
  discounting: 1.0
  normalize_observations: True
  reward_scaling: 1
  gae_lambda: 0.95

  # eval
  num_evals: 20
  num_eval_envs: 2048
  deterministic_eval: True

ppo:
  unroll_length: 10
  batch_size: 200
  num_minibatches: 1
  num_updates_per_batch: 16
  num_envs: 200 

  learning_rate: 3e-4

diffrl_shac:
  unroll_length: 32
  batch_size: 256
  num_minibatches: 1
  num_updates_per_batch: 16
  num_envs: 256 

  actor_lr: 2e-3
  critic_lr: 1.5e-3

ihac:
  start_unroll_length: 20
  end_unroll_length: 0
  unroll_length_step: -1
  batch_size: 200
  num_minibatches: 1
  num_updates_per_batch: 16
  num_envs: 200 

  actor_lr: 1.5e-3
  critic_lr: 1.5e-3

hybrid_apg:
  unroll_length: 20
  batch_size: 200
  num_minibatches: 1
  num_updates_per_batch: 2
  num_envs: 200 

  actor_lr: 1.5e-3
  critic_lr: 1.5e-3

dual_apg:
  unroll_length: 20
  batch_size: 200
  num_minibatches: 1
  num_updates_per_batch: 2
  num_envs: 200 

  actor_lr: 1.5e-3
  critic_lr: 1.5e-3

unroll_apg:
  batch_size: 200
  num_envs: 200

  learning_rate: 1.5e-3

deterministic_apg:
  batch_size: 200
  num_envs: 200

  learning_rate: 1.5e-3
