name: inverted_pendulum
include_time: false
dtype: float64

kwargs:
  eps: 1e-6
  reward_shaping: false

config:
  # schedule parameters
  num_timesteps: 2_000_000

  # environment wrapper
  episode_length: 1_000
  action_repeat: 1

  # network parameters
  actor_grad_norm: 1.0
  critic_grad_norm: 1.0
  lr_schedule: constant
  policy_hidden_layer_sizes: [64, 64] # [32, 32, 32, 32]
  value_hidden_layer_sizes: [64, 64] # [256, 256, 256, 256, 256]
  tau: 0.8
  betas: [0.7, 0.95]

  # SHAC params
  entropy_cost: 1e-2
  discounting: 0.97
  normalize_observations: True
  gae_lambda: 0.95
  actor_xi: 0.95

  # eval
  num_evals: 20
  num_eval_envs: 1024
  deterministic_eval: True

# sac:
#   batch_size: 512
#   grad_updates_per_step: 64
#   num_envs: 128

#   max_replay_size: 1048576
#   min_replay_size: 8192

#   learning_rate: 6e-4

ppo:
  num_envs: 2048
  learning_rate: 3e-4
  unroll_length: 5
  batch_size: 1024
  num_minibatches: 32
  num_updates_per_batch: 4
  reward_scaling: 10

diffrl_shac:
  unroll_length: 32
  batch_size: 512
  num_minibatches: 4
  num_updates_per_batch: 16
  num_envs: 2048

  reward_scaling: 1

  actor_lr: 2e-3
  critic_lr: 1e-3

clipped_shac:
  unroll_length: 32
  batch_size: 512
  num_minibatches: 4
  num_updates_per_batch: 16
  num_envs: 2048

  reward_scaling: 1

  actor_lr: 2e-3
  critic_lr: 1e-3

generalized_hac:
  unroll_length: 32
  batch_size: 512
  num_minibatches: 4
  num_updates_per_batch: 16
  num_envs: 2048

  reward_scaling: 1

  actor_lr: 2e-3
  critic_lr: 1e-3

unroll_apg:
  batch_size: 1024
  num_envs: 1024

  learning_rate: 1e-3

deterministic_apg:
  batch_size: 2048
  num_envs: 2048

  learning_rate: 2e-4
